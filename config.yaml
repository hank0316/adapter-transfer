trainer:
  learning_rate: 1.0e-4
  train_step: 2000
  logging_step: 250
  eval_step: 250
  save_step: 250
  save_total_limit: 1
  # Arguments for trainer
  load_best_model_at_end: True
  overwrite_output_dir: True
  remove_unused_columns: False
  # Path to store the checkpoints
  ckpt_path: 'checkpoint-best'
  # Path of output log
  logging_dir: 'log'
  # Trainer will store model according to this metric
  metric_for_best_model : 'eval_'

model:
  name: 'roberta-base'

exp_setup:
  chain_length: 3
  # For MTL
  num_multi_task: 3
  mtl_batch_size: 8
  tasks:
    name: ['rte', 'stsb', 'mrpc', 'cola']
    metrics:
      stsb: 'spearmanr'
      rte: 'acc'
      mrpc: 'acc'
      cola: 'matthews'
